---
title: "The Nyström Method for Convex Loss Functions"
authors: "A. Della Vecchia, J. Mourtada, E. De Vito, L. Rosasco"
venue: "International Conference on Artificial Intelligence and Statistics (AISTATS)"
date: 2021-03-18
year: 2021
paperurl: "https://scholar.google.it/citations?view_op=view_citation&hl=en&user=aaeUheEAAAAJ&citation_for_view=aaeUheEAAAAJ:u5HHmVD_uO8C"
pdf: "/files/papers/DellaVecchia_random.pdf"
selected: true
excerpt: >-
  **Abstract:** We study a natural extension of classical empirical risk minimization, where the
  hypothesis space is a random subspace of a given space. In particular, we consider possibly
  data-dependent subspaces spanned by a random subset of the data, recovering as a special case
  Nyström approaches for kernel methods. Considering random subspaces naturally leads to
  computational savings, but the question is whether the corresponding learning accuracy is
  degraded. These statistical–computational tradeoffs have been recently explored for the least
  squares loss and self-concordant loss functions, such as the logistic loss. Here, we extend
  these results to convex Lipschitz loss functions, that might not be smooth, such as the hinge
  loss used in support vector machines. This extension requires developing new proofs, using
  different technical tools. Our main results show the existence of different settings, depending
  on how hard the learning problem is, for which computational efficiency can be improved with no
  loss in performance. Theoretical results are illustrated with simple numerical experiments.
---
